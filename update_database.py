"""Add new articles to the vector database"""

import logging
import os
import datetime
from dotenv import load_dotenv
from typing import Final
from tqdm.auto import tqdm
from abstrag.core.database import (
    PostgresParams,
    get_article_id_data,
    open_db_connection,
    insert_embedding_data,
)
from abstrag.ingest import retrieve_arxiv_metadata, paper_html_to_markdown
from abstrag.core.embedding import (
    PaperEmbedding,
    ChunkParams,
    chunk_document,
    document_embedding,
)
from abstrag.config import get_config

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

load_dotenv(".env")
config_dict = get_config()
if config_dict:
    from abstrag.config import IngestionConfig
    config_ingestion = IngestionConfig(**config_dict["ingestion"])
else:
    raise ValueError("Failed to load configuration")

MAX_RESULTS_ARXIV = config_ingestion.max_documents_arxiv
CHUNK_SIZE = config_ingestion.chunk_size
CHUNK_OVERLAP = config_ingestion.chunk_overlap
CHUNK_METHOD: Final = config_ingestion.chunk_method
EMBEDDING_MODEL_NAME: Final = config_ingestion.embedding_model_name
TABLE_EMBEDDING_ARTICLE = f"embedding_article_{EMBEDDING_MODEL_NAME}".replace("-", "_")
TABLE_EMBEDDING_ABSTRACT = f"embedding_abstract_{EMBEDDING_MODEL_NAME}".replace(
    "-", "_"
)

# Open connection to database
postgres_connection_params = PostgresParams(
    host=os.environ["POSTGRES_HOST"],
    port=os.environ["POSTGRES_PORT"],
    user=os.environ["POSTGRES_USER"],
    pwd=os.environ["POSTGRES_PWD"],
    database=os.environ["POSTGRES_DB"],
)
conn = open_db_connection(connection_params=postgres_connection_params, autocommit=True)

if MAX_RESULTS_ARXIV > 0:
    # Get list of article ids already present in database
    article_ids_stored = get_article_id_data(
        conn=conn, table_name=TABLE_EMBEDDING_ARTICLE
    )

    # Fetch new papers from arxiv
    # Exclude papers published within the last 7 days as they may not have HTML versions yet
    # HTML versions typically take a few days to be generated by arXiv
    min_paper_age_days = 7
    cutoff_date = datetime.datetime.now(datetime.timezone.utc) - datetime.timedelta(days=min_paper_age_days)
    
    logger.info(f"Fetching up to {MAX_RESULTS_ARXIV} papers from arXiv (excluding {len(article_ids_stored)} already in database)")
    logger.info(f"Excluding papers published after {cutoff_date.strftime('%Y-%m-%d')} (papers need at least {min_paper_age_days} days for HTML version availability)")
    
    # Fetch and process papers incrementally to avoid rate limiting
    # We'll fetch papers in batches and process them as we go
    markdown_text = []
    failed_papers = []
    papers_processed = 0
    
    # Estimate how many papers we need to fetch to get enough with HTML versions
    # Limit to 100 papers (one API page) to avoid rate limiting
    # If we need more papers, they can be fetched in a subsequent run
    max_papers_to_check = min(int(MAX_RESULTS_ARXIV * 2), 100) if MAX_RESULTS_ARXIV > 0 else 100
    logger.info(f"Will check up to {max_papers_to_check} papers from arXiv (limited to avoid rate limiting)")
    
    try:
        metadata = retrieve_arxiv_metadata(
            max_results=max_papers_to_check, 
            exclude_ids=article_ids_stored, 
            last_date=cutoff_date,
            verbose=True
        )
        logger.info(f"Retrieved {len(metadata)} candidate papers from arXiv")
        
        # Process documents and stop when we have enough successful papers
        for paper_id in tqdm(metadata, total=len(metadata), desc="Processing papers"):
            # Stop if we've reached the target number of successful papers
            if MAX_RESULTS_ARXIV > 0 and len(markdown_text) >= MAX_RESULTS_ARXIV:
                logger.info(f"Reached target of {MAX_RESULTS_ARXIV} successfully processed papers")
                break
                
            papers_processed += 1
            article_markdown = paper_html_to_markdown(paper_id=paper_id, verbose=False)
            if article_markdown:
                dict_markdown = dict(id=paper_id["id"], article=article_markdown)
                dict_markdown["abstract"] = [
                    meta["summary"] for meta in metadata if meta["id"] == paper_id["id"]
                ]
                markdown_text.append(dict_markdown)
            else:
                failed_papers.append(paper_id["id"])
                if len(failed_papers) <= 5:  # Only log first few failures to avoid spam
                    logger.debug(f"HTML version not available for {paper_id['id']}")
        
    except Exception as e:
        logger.error(f"Error fetching papers from arXiv: {e}")
        if "429" in str(e) or "rate limit" in str(e).lower():
            logger.warning("Rate limited by arXiv API. Consider reducing max_documents_arxiv or waiting before retrying.")
        raise
    
    if failed_papers:
        logger.info(f"Skipped {len(failed_papers)} papers without HTML versions (processed {papers_processed} total)")
        if len(failed_papers) <= 10:
            logger.debug(f"Skipped paper IDs: {failed_papers[:10]}")
    
    logger.info(f"Successfully processed {len(markdown_text)} papers with HTML versions")

    # Chunk and embed documents
    chunk_parameters = ChunkParams(
        method=CHUNK_METHOD, size=CHUNK_SIZE, overlap=CHUNK_OVERLAP
    )

    list_article_embeddings = []
    list_abstract_embeddings = []
    embedding_dimension = 0
    for article in tqdm(markdown_text, total=len(markdown_text)):
        article_id = article["id"]
        document = article["article"]
        abstract = article["abstract"][0] if article["abstract"] else ""

        # Chunk document
        document_chunks = chunk_document(
            document=document, chunk_params=chunk_parameters
        )

        # Document embedding
        article_embedding = document_embedding(
            chunks=document_chunks, embedding_model=EMBEDDING_MODEL_NAME
        )

        # Abstract embedding
        abstract_embedding = document_embedding(
            chunks=[abstract], embedding_model=EMBEDDING_MODEL_NAME
        )

        if embedding_dimension == 0:
            embedding_dimension = article_embedding["dimension"]

        # Format output
        for i in range(len(article_embedding["content"])):
            row_store = PaperEmbedding(
                id=article_id,
                content=article_embedding["content"][i],
                embeddings=article_embedding["embedding"][i, :],
            )
            list_article_embeddings.append(row_store)

        for i in range(len(abstract_embedding["content"])):
            row_store = PaperEmbedding(
                id=article_id,
                content=abstract_embedding["content"][i],
                embeddings=abstract_embedding["embedding"][i, :],
            )
            list_abstract_embeddings.append(row_store)

    # Store documents and embeddings in the database
    if list_article_embeddings:
        insert_embedding_data(
            conn=conn,
            table_name=TABLE_EMBEDDING_ARTICLE,
            paper_embedding=list_article_embeddings,
        )
        logger.info(f"Successfully inserted {len(list_article_embeddings)} article embeddings")
    else:
        logger.warning("No article embeddings to insert")
    
    if list_abstract_embeddings:
        insert_embedding_data(
            conn=conn,
            table_name=TABLE_EMBEDDING_ABSTRACT,
            paper_embedding=list_abstract_embeddings,
        )
        logger.info(f"Successfully inserted {len(list_abstract_embeddings)} abstract embeddings")
    else:
        logger.warning("No abstract embeddings to insert")
