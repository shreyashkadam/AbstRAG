# Evaluation Framework

This framework benchmarks the performance of **abstRAG** against standard baselines to ensure high-quality retrieval and generation.

## Benchmarked Methods

We compare three distinct approaches:

1.  **B1_BM25 (Baseline)**: Traditional keyword search using the BM25 algorithm. Fast but lacks semantic understanding.
2.  **B2_SingleStep (Baseline)**: Standard RAG. Semantic search against the entire database of chunks.
3.  **OurMethod_2Step (abstRAG)**: A hierarchical approach:
    - **Step 1**: Semantic search on *abstracts* to identify relevant papers.
    - **Step 2**: Semantic search on *chunks* only within those identified papers.

## Metrics

### 1. Retrieval Quality
- **Precision@k**: How many retrieved documents are relevant?
- **Recall@k (Hit Rate)**: Did we find the target document?
- **MRR (Mean Reciprocal Rank)**: How high up is the first relevant result?
- **nDCG**: Measures ranking quality (relevant items should be at the top).

### 2. Answer Quality (LLM-as-a-Judge)
We use a stronger LLM (e.g., Llama 3.1 70B) to evaluate the answers generated by the system:
- **Relevance**: Does the answer address the user's question?
- **Factuality**: Is the answer supported by the retrieved context?

### 3. Efficiency
- **Latency**: End-to-end response time.

## Running the Evaluation

The evaluation pipeline is automated via scripts in the `scripts/` directory.

### Step 1: Generate Test Questions
Create synthetic questions based on the papers in your database:

```bash
# Generates questions from papers currently in your DB
python scripts/generate_evaluation_questions_from_db.py
```

### Step 2: Run Benchmarks
Run the comparison script. This will query all methods and log metrics.

```bash
python scripts/evaluate_baselines.py
```
*Results are saved to `reports/evaluation/results/`.*

### Step 3: Analyze Results
Aggregate the raw data into summary tables and plots:

```bash
# Summarize results
python scripts/aggregate_results.py reports/evaluation/results/baseline_evaluation_TIMESTAMP.csv

# Generate plots
python scripts/generate_plots.py reports/evaluation/results/baseline_evaluation_TIMESTAMP.csv
```

## Visualizing Results

Generated plots are saved in `reports/evaluation/figures/`. These include:
- Precision-Recall curves.
- Latency comparison histograms.
- Bar charts for Answer Relevance.
